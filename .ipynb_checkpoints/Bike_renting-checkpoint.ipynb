{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement  \n",
    "\n",
    "The goal is to forecast the demand for bikes in dependency of weather conditions like outside temperature and calendric informations e.g. holidays. These information and the demand structure is provided in a set with two years of daily historic data.  \n",
    "The demand is given as the total daily demand and as a split for registered users and casual users. To increase the quality of the prediction registered user demand and casual user demand will be predicted separately in step two.  \n",
    "To make predictions machine learning is used to train regressors. Scikit-Learn recommends a support vector regressor (SVR) for this kind of problem and data amount. In addition a deep neuronal network (DNN) regressor is trained for comparison. To find the hyper-parameters for these regressors grid search and randomized search are utilized. Due to the small dataset cross validation is applied.    \n",
    "\n",
    "> http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR  \n",
    "> https://github.com/tensorflow/skflow/blob/master/g3doc/api_docs/python/estimators.md  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching Dataset\n",
    "\n",
    "bike_data = pd.read_csv(\"day.csv\", header=0)\n",
    "\n",
    "print(\"Data read successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extracting\n",
    "\n",
    "feature_cols = bike_data.columns[:-3]  # all columns but last are features\n",
    "target_col = bike_data.columns[-1]  # last column is the target\n",
    "\n",
    "print (\"Feature column(s):\\n{}\\n\".format(feature_cols))\n",
    "print (\"Target column:\\n{}\".format(target_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function to Calculate Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit(y,y_cap):\n",
    "    return 3 * np.minimum(y[::1], y_cap[::1]) - 2 * y_cap[::1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Convert from percentage to Actual Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToPrediction(data,percentage_predictions):\n",
    "    demand = np.around(data + (np.multiply(data, percentage_predictions)/100))\n",
    "    demand[demand <0 ] = 0\n",
    "    return demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the base model the demand for today is the previous days demand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = bike_data[target_col][365:731]  # corresponding targets\n",
    "y_actual = y_actual.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_staged = y_actual.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data.insert(0, bike_data[target_col][364])\n",
    "data.insert(0, bike_data[target_col][363])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_df = pd.concat([pd.DataFrame(data), y_staged], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_df.drop(y_predicted_df.tail(2).index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = y_predicted_df[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Base Model Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(profit(y_actual,y_predicted).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw_train = pd.read_csv(\"train.csv\", header=0)\n",
    "X_raw_test  = pd.read_csv(\"test.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"temp\",\"hum\", \"windspeed\" ,\"cnt_normal\",\"week_moving_avg_normal\",\"season_1\",\"season_2\",\"season_3\",\"season_4\",\"mnth_1\",\"mnth_2\",\"mnth_3\",\"mnth_4\",\"mnth_5\",\"mnth_6\",\"mnth_7\",\"mnth_8\",\"mnth_9\",\"mnth_10\",\"mnth_11\",\"mnth_12\",\"holiday_1\",\"holiday_2\",\"weekday_1\",\"weekday_2\",\"weekday_3\",\"weekday_4\",\"weekday_5\",\"weekday_6\",\"weekday_7\",\"workingday_1\",\"workingday_2\",\"weathersit_1\",\"weathersit_2\",\"weathersit_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"atemp\",\"hum\", \"windspeed\" ,\"cnt_normal\",\"week_moving_avg_normal\",\"season_1\",\"season_2\",\"season_3\",\"season_4\",\"mnth_1\",\"mnth_2\",\"mnth_3\",\"mnth_4\",\"mnth_5\",\"mnth_6\",\"mnth_7\",\"mnth_8\",\"mnth_9\",\"mnth_10\",\"mnth_11\",\"mnth_12\",\"holiday_1\",\"holiday_2\",\"weekday_1\",\"weekday_2\",\"weekday_3\",\"weekday_4\",\"weekday_5\",\"weekday_6\",\"weekday_7\",\"workingday_1\",\"workingday_2\",\"weathersit_1\",\"weathersit_2\",\"weathersit_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_raw_train[cols].values.tolist()\n",
    "y_train_df = X_raw_train[['target']]\n",
    "y_train = y_train_df['target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_raw_test[cols].values.tolist()\n",
    "y_test_df = X_raw_test[['target']]\n",
    "y_test = y_test_df['target'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate dataset with percentage change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"processed_Data.csv\", header=0)\n",
    "data['instant'] = data['instant'] % 30\n",
    "X_raw_train = data[0:359]\n",
    "X_raw_test  = data[359:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =[\n",
    "       \"season__1\",\"season__2\",\"season__3\",\"season__4\",\"season__5\",\n",
    "       \"weathersit__1\",\"weathersit__2\",\"weathersit__3\",\"weathersit__4\",\"weathersit__5\",\n",
    "        \"cnt__1\",\n",
    "        \"atemp\",\"hum\",\"windspeed\",\n",
    "        \"mnth\",\"instant\",\"holiday\",\"weekday\",\"workingday\",\n",
    "        \"moving_avg_weekly_cnt\"]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_raw_train[cols].values.tolist()\n",
    "y_train_df = X_raw_train[['demand_pc_inc']]\n",
    "y_train = y_train_df['demand_pc_inc'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_raw_test[cols].values.tolist()\n",
    "y_test_df = X_raw_test[['demand_pc_inc']]\n",
    "y_test = y_test_df['demand_pc_inc'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cnt = data['cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_predictions = data_cnt[359:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_for_calculations = data_cnt[357:723].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation metrics\n",
    "from sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor\n",
      "1571240.0\n",
      "GradientBoostingRegressor\n",
      "1585123.0\n",
      "AdaBoostRegressor\n",
      "1500697.0\n",
      "BaggingRegressor\n",
      "1588207.0\n",
      "SVR\n",
      "1438256.0\n",
      "KNeighborsRegressor\n",
      "1445710.0\n"
     ]
    }
   ],
   "source": [
    "models=[RandomForestRegressor(),GradientBoostingRegressor(),AdaBoostRegressor(),BaggingRegressor(),SVR(),KNeighborsRegressor()]\n",
    "model_names=['RandomForestRegressor','GradientBoostingRegressor','AdaBoostRegressor','BaggingRegressor','SVR','KNeighborsRegressor']\n",
    "rmsle=[]\n",
    "d={}\n",
    "for model in range (len(models)):\n",
    "    clf=models[model]\n",
    "    print(model_names[model])\n",
    "    clf.fit(X_train,y_train)\n",
    "    test_pred=clf.predict(X_test)\n",
    "    model_predictions = convertToPrediction(y_for_calculations,test_pred)\n",
    "    print(profit(actual_predictions,model_predictions).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW LET'S Dig deeper into each of these ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The regressors are trained using randomized search and cross-validation to identify the area of the best parameters. Then a grid search is used to tune parameter values of the regressor functions.\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1547591.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for random forest regresion.\n",
    "no_of_test=[500]\n",
    "params_dict={'n_estimators':no_of_test,'n_jobs':[-1],'max_features':[\"auto\",'sqrt','log2'],'max_depth':[10,20,30]}\n",
    "clf_rf=GridSearchCV(estimator=RandomForestRegressor(),param_grid=params_dict,scoring='neg_mean_squared_error')\n",
    "clf_rf.fit(X_train,y_train)\n",
    "pred_rf=clf_rf.predict(X_test)\n",
    "model_predictions_rf = convertToPrediction(y_for_calculations,pred_rf)\n",
    "print(profit(actual_predictions,model_predictions_rf).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 500, 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1576971.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "#regressor = RandomForestRegressor(random_state = 0, max_depth = 30, n_estimators = 500, max_features = 'log2')\n",
    "regressor = RandomForestRegressor()\n",
    "estimator_rf = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_rf.fit(X_train, y_train)\n",
    "pred_rf = estimator_rf.predict(X_test)\n",
    "model_predictions_rf = convertToPrediction(y_for_calculations,pred_rf)\n",
    "print(profit(actual_predictions,model_predictions_rf).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1452701.0\n"
     ]
    }
   ],
   "source": [
    "#for Gradient Boosting regresion.\n",
    "no_of_estimators=[100,200,300,400,500]\n",
    "params_dict={'n_estimators':no_of_estimators,'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "             'max_features':[\"auto\",'sqrt','log2'],'max_depth':[10,20,30,40,50]}\n",
    "clf_gbr=GridSearchCV(estimator=GradientBoostingRegressor(),param_grid=params_dict,scoring='neg_mean_squared_error')\n",
    "clf_gbr.fit(X_train,y_train)\n",
    "pred=clf_gbr.predict(X_test)\n",
    "model_predictions = convertToPrediction(y_for_calculations,pred)\n",
    "print(profit(actual_predictions,model_predictions).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'learning_rate': 0.2, 'max_depth': 40, 'max_features': 'log2', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf_gbr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1550460.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=500,max_features= 'log2', learning_rate=0.1, max_depth = 10)\n",
    "\n",
    "estimator_gbr = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', gbr)\n",
    "    ]\n",
    ")\n",
    "estimator_gbr.fit(X_train, y_train)\n",
    "pred_gbr = estimator_gbr.predict(X_test)\n",
    "model_predictions_gbr = convertToPrediction(y_for_calculations,pred_gbr)\n",
    "print(profit(actual_predictions,model_predictions_gbr).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADA Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503583.0\n"
     ]
    }
   ],
   "source": [
    "### ADA Boost Regressor\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "#regressor = RandomForestRegressor(random_state = 0, max_depth = 30, n_estimators = 500, max_features = 'log2')\n",
    "regressor = AdaBoostRegressor()\n",
    "estimator_ada = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_ada.fit(X_train, y_train)\n",
    "pred_ada = estimator_ada.predict(X_test)\n",
    "model_predictions_ada = convertToPrediction(y_for_calculations,pred_ada)\n",
    "print(profit(actual_predictions,model_predictions_ada).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1588304.0\n"
     ]
    }
   ],
   "source": [
    "### Bagging Regressor\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "#regressor = RandomForestRegressor(random_state = 0, max_depth = 30, n_estimators = 500, max_features = 'log2')\n",
    "regressor = BaggingRegressor()\n",
    "estimator_bagging = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_bagging.fit(X_train, y_train)\n",
    "pred_bagging = estimator_bagging.predict(X_test)\n",
    "model_predictions_bagging = convertToPrediction(y_for_calculations,pred_bagging)\n",
    "print(profit(actual_predictions,model_predictions_bagging).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1445710.0\n"
     ]
    }
   ],
   "source": [
    "### KNN Regressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "\n",
    "regressor = KNeighborsRegressor()\n",
    "estimator_knn = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_knn.fit(X_train, y_train)\n",
    "pred_knn = estimator_knn.predict(X_test)\n",
    "model_predictions_knn = convertToPrediction(y_for_calculations,pred_knn)\n",
    "print(profit(actual_predictions,model_predictions_knn).sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training SVR\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score SVR: -0.003305\n",
      "RMSE SVR: 1318.397271\n"
     ]
    }
   ],
   "source": [
    "# Validation SVR\n",
    "\n",
    "pred_svr = svr.predict(X_test)\n",
    "score_svr = r2_score(y_test, pred_svr)\n",
    "rmse_svr = sqrt(mean_squared_error(y_test, pred_svr))\n",
    "\n",
    "print(\"Score SVR: %f\" % score_svr)\n",
    "print(\"RMSE SVR: %f\" % rmse_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning SVR with GridSearch\n",
    "\n",
    "tuned_parameters = [{'C': [1000, 3000, 10000], \n",
    "                     'kernel': ['linear', 'rbf']}\n",
    "                   ]\n",
    "\n",
    "#svr_tuned = GridSearchCV(SVR (C=1), param_grid = tuned_parameters, scoring = 'mean_squared_error') #default 3-fold cross-validation, score method of the estimator\n",
    "svr_tuned_GS = GridSearchCV(SVR (C=1), param_grid = tuned_parameters, scoring = 'r2', n_jobs=-1) #default 3-fold cross-validation, score method of the estimator\n",
    "\n",
    "svr_tuned_GS.fit(X_train, y_train)\n",
    "\n",
    "print (svr_tuned_GS)\n",
    "print ('\\n' \"Best parameter from grid search: \" + str(svr_tuned_GS.best_params_) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_tuned_pred_GS = svr_tuned_GS.predict(X_test)\n",
    "\n",
    "score_svr_tuned_GS = r2_score(y_test, svr_tuned_pred_GS)\n",
    "rmse_svr_tuned_GS = sqrt(mean_squared_error(y_test, svr_tuned_pred_GS))\n",
    "\n",
    "print(\"SVR Results\\n\")\n",
    "\n",
    "print(\"Score SVR: %f\" % score_svr)\n",
    "print(\"Score SVR tuned GS: %f\" % score_svr_tuned_GS)\n",
    "\n",
    "print(\"\\nRMSE SVR: %f\" % rmse_svr)\n",
    "print(\"RMSE SVR tuned GS: %f\" % rmse_svr_tuned_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_tuned_pred_GS\n",
    "\n",
    "\n",
    "##Profit Calculation for pct approach\n",
    "model_predictions = convertToPrediction(y_for_calculations,svr_tuned_pred_GS)\n",
    "print(profit(actual_predictions,model_predictions).sum())\n",
    "\n",
    "#Profit is just 1.26million!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR tuned with RandomizesSearch\n",
    "# may take a while!\n",
    "\n",
    "# Parameters\n",
    "param_dist = {  'C': sp_uniform (1000, 10000), \n",
    "                'kernel': ['rbf']\n",
    "             }\n",
    "\n",
    "n_iter_search = 1\n",
    "\n",
    "# MSE optimized\n",
    "#SVR_tuned_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'mean_squared_error', n_iter=n_iter_search)\n",
    "\n",
    "# R^2 optimized\n",
    "SVR_tuned_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'r2', n_iter=n_iter_search)\n",
    "\n",
    "# Fit\n",
    "SVR_tuned_RS.fit(X_train, y_train)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('best CV score from grid search: {0:f}'.format(SVR_tuned_RS.best_score_))\n",
    "print('corresponding parameters: {}'.format(SVR_tuned_RS.best_params_))\n",
    "\n",
    "# Predict and score\n",
    "predict = SVR_tuned_RS.predict(X_test)\n",
    "\n",
    "score_svr_tuned_RS = r2_score(y_test, predict)\n",
    "rmse_svr_tuned_RS = sqrt(mean_squared_error(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVR Results\\n')\n",
    "\n",
    "print(\"Score SVR: %f\" % score_svr)\n",
    "print(\"Score SVR tuned GS: %f\" % score_svr_tuned_GS)\n",
    "print(\"Score SVR tuned RS: %f\" % score_svr_tuned_RS)\n",
    "\n",
    "print(\"\\nRMSE SVR: %f\" % rmse_svr)\n",
    "print(\"RMSE SVR tuned GS: %f\" % rmse_svr_tuned_GS)\n",
    "print(\"RMSE SVR tuned RS: %f\" % rmse_svr_tuned_RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Profit Calculation for pct approach\n",
    "model_predictions = convertToPrediction(y_for_calculations,predict)\n",
    "print(profit(actual_predictions,model_predictions).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from time import time\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_model = MLPRegressor(hidden_layer_sizes=(5,),\n",
    "                                       activation='relu',\n",
    "                                       solver='adam',\n",
    "                                       learning_rate='adaptive',\n",
    "                                       max_iter=15000,\n",
    "                                       learning_rate_init=0.01,\n",
    "                                       alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = int(time() * 1000)\n",
    "bike_model.fit(X_train, y_train)\n",
    "end_time = int(time() * 1000)\n",
    "logging.debug('Finished training universal model')\n",
    "logging.debug('Training took {} ms'.format(end_time - start_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dnn = bike_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions_dnn = convertToPrediction(y_for_calculations,pred_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(profit(actual_predictions,model_predictions_dnn).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averageing best predictions from different regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1547627.0\n"
     ]
    }
   ],
   "source": [
    "w = [1, 1, 1, 1, 1,1,1]  # weights\n",
    "pred_agg = np.c_[pred_rf,pred_gbr,pred_ada,pred_bagging,pred_knn,pred_svr,pred_dnn ]\n",
    "pred_avr = np.average(pred_agg, axis=1, weights=w)\n",
    "model_predictions_avr = convertToPrediction(y_for_calculations,pred_avr)\n",
    "print(profit(actual_predictions,model_predictions_avr).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble : Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_rf = estimator_rf.predict(X_train)\n",
    "pred_train_gbr = estimator_gbr.predict(X_train)\n",
    "pred_train_ada = estimator_rf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = pd.DataFrame({'rf':pred_train_rf, 'gbr':pred_train_gbr, 'ada':pred_train_ada, 'true':y_test})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
