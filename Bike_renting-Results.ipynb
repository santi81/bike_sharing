{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement  \n",
    "\n",
    "The goal is to forecast the demand for bikes in dependency of weather conditions like outside temperature and calendric informations e.g. holidays. These information and the demand structure is provided in a set with two years of daily historic data.  \n",
    "The demand is given as the total daily demand and as a split for registered users and casual users. To increase the quality of the prediction registered user demand and casual user demand will be predicted separately in step two.  \n",
    "To make predictions machine learning is used to train regressors. Scikit-Learn recommends a support vector regressor (SVR) for this kind of problem and data amount. In addition a deep neuronal network (DNN) regressor is trained for comparison. To find the hyper-parameters for these regressors grid search and randomized search are utilized. Due to the small dataset cross validation is applied.    \n",
    "\n",
    "> http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR  \n",
    "> https://github.com/tensorflow/skflow/blob/master/g3doc/api_docs/python/estimators.md  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fetching Dataset\n",
    "\n",
    "bike_data = pd.read_csv(\"day.csv\", header=0)\n",
    "\n",
    "print(\"Data read successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>331</td>\n",
       "      <td>654</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>131</td>\n",
       "      <td>670</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>120</td>\n",
       "      <td>1229</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>108</td>\n",
       "      <td>1454</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>82</td>\n",
       "      <td>1518</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1        0        6           0   \n",
       "1        2  2011-01-02       1   0     1        0        0           0   \n",
       "2        3  2011-01-03       1   0     1        0        1           1   \n",
       "3        4  2011-01-04       1   0     1        0        2           1   \n",
       "4        5  2011-01-05       1   0     1        0        3           1   \n",
       "\n",
       "   weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
       "0           2  0.344167  0.363625  0.805833   0.160446     331         654   \n",
       "1           2  0.363478  0.353739  0.696087   0.248539     131         670   \n",
       "2           1  0.196364  0.189405  0.437273   0.248309     120        1229   \n",
       "3           1  0.200000  0.212122  0.590435   0.160296     108        1454   \n",
       "4           1  0.226957  0.229270  0.436957   0.186900      82        1518   \n",
       "\n",
       "    cnt  \n",
       "0   985  \n",
       "1   801  \n",
       "2  1349  \n",
       "3  1562  \n",
       "4  1600  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):\n",
      "Index(['instant', 'dteday', 'season', 'yr', 'mnth', 'holiday', 'weekday',\n",
      "       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed'],\n",
      "      dtype='object')\n",
      "\n",
      "Target column:\n",
      "cnt\n"
     ]
    }
   ],
   "source": [
    "# Extracting\n",
    "\n",
    "feature_cols = bike_data.columns[:-3]  # all columns but last are features\n",
    "target_col = bike_data.columns[-1]  # last column is the target\n",
    "\n",
    "print (\"Feature column(s):\\n{}\\n\".format(feature_cols))\n",
    "print (\"Target column:\\n{}\".format(target_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function to Calculate Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit(y,y_cap):\n",
    "    return 3 * np.minimum(y[::1], y_cap[::1]) - 2 * y_cap[::1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Convert from percentage to Actual Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToPrediction(data,percentage_predictions):\n",
    "    demand = np.around(data + (np.multiply(data, percentage_predictions)/100))\n",
    "    demand[demand <0 ] = 0\n",
    "    return demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RMSLE Scorer\n",
    "One common way to evaluate regression model is through calculating MSE or RMSE. In this particular project , the metric to evaluate our model is Root Mean Square Logarithmic Error (RMSLE). RMSLE is particularly helpful when you want to penalize an under-predicted estimate greater than an over-predicted estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_):\n",
    "    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n",
    "    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n",
    "    calc = (log1 - log2) ** 2\n",
    "    return np.sqrt(np.mean(calc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the base model the demand for today is the previous days demand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = bike_data[target_col][365:731]  # corresponding targets\n",
    "y_actual = y_actual.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_staged = y_actual.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data.insert(0, bike_data[target_col][364])\n",
    "data.insert(0, bike_data[target_col][363])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_df = pd.concat([pd.DataFrame(data), y_staged], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_df.drop(y_predicted_df.tail(2).index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = y_predicted_df[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Base Model Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1442972\n"
     ]
    }
   ],
   "source": [
    "print(profit(y_actual,y_predicted).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset with percentage change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_raw_test[cols].values.tolist()\n",
    "y_test_df = X_raw_test[['target']]\n",
    "y_test = y_test_df['target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"processed_Data.csv\", header=0)\n",
    "data['instant'] = data['instant'] % 30\n",
    "X_raw_train = data[0:359]\n",
    "X_raw_test  = data[359:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =[\n",
    "       \"season__1\",\"season__2\",\"season__3\",\"season__4\",\"season__5\",\n",
    "       \"weathersit__1\",\"weathersit__2\",\"weathersit__3\",\"weathersit__4\",\"weathersit__5\",\n",
    "        \"cnt__1\",\n",
    "        \"atemp\",\"hum\",\"windspeed\",\n",
    "        \"mnth\",\"instant\",\"holiday\",\"weekday\",\"workingday\",\n",
    "        \"moving_avg_weekly_cnt\"]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_raw_train[cols].values.tolist()\n",
    "y_train_df = X_raw_train[['demand_pc_inc']]\n",
    "y_train = y_train_df['demand_pc_inc'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_raw_test[cols].values.tolist()\n",
    "y_test_df = X_raw_test[['demand_pc_inc']]\n",
    "y_test = y_test_df['demand_pc_inc'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cnt = data['cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_predictions = data_cnt[359:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_for_calculations = data_cnt[357:723].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensemble Methods\n",
    "\n",
    "Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking). Ensemble methods can be divided into two groups: sequential ensemble methods where the base learners are generated sequentially (e.g. AdaBoost) and parallel ensemble methods where the base learners are generated in parallel (e.g. Random Forest). The basic motivation of sequential methods is to exploit the dependence between the base learners since the overall performance can be boosted by weighing previously mislabeled examples with higher weight. The basic motivation of parallel methods is to exploit independence between the base learners since the error can be reduced dramatically by averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting\n",
    "Boosting refers to a family of algorithms that are able to convert weak learners to strong learners. The main principle of boosting is to fit a sequence of weak learners (models that are only slightly better than random guessing, such as small decision trees) to weighted versions of the data, where more weight is given to examples that were mis-classified by earlier rounds. The predictions are then combined through a weighted majority vote (classification) or a weighted sum (regression) to produce the final prediction. The principal difference between boosting and the committee methods such as bagging is that base learners are trained in sequence on a weighted version of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adaptive Boosting\n",
    "\n",
    "The algorithm below describes the most widely used form of boosting algorithm called AdaBoost, which stands for adaptive boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508066.0\n"
     ]
    }
   ],
   "source": [
    "### ADA Boost Regressor\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "#regressor = RandomForestRegressor(random_state = 0, max_depth = 30, n_estimators = 500, max_features = 'log2')\n",
    "regressor = AdaBoostRegressor()\n",
    "estimator_ada = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_ada.fit(X_train, y_train)\n",
    "pred_ada = estimator_ada.predict(X_test)\n",
    "model_predictions_ada = convertToPrediction(y_for_calculations,pred_ada)\n",
    "print(profit(actual_predictions,model_predictions_ada).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1456869.0\n"
     ]
    }
   ],
   "source": [
    "num_est = [1, 2, 3, 10,50,100,200,300,500]\n",
    "learning_rate = [0.01,0.1,0.5,1]\n",
    "loss = ['linear', 'square', 'exponential']\n",
    "params_dict={'n_estimators':num_est,'learning_rate': learning_rate,'loss':loss}\n",
    "clf_ada=GridSearchCV(estimator=AdaBoostRegressor(),param_grid=params_dict,scoring='neg_mean_squared_error')\n",
    "clf_ada.fit(X_train,y_train)\n",
    "pred_ada=clf_ada.predict(X_test)\n",
    "model_predictions_ada = convertToPrediction(y_for_calculations,pred_ada)\n",
    "print(profit(actual_predictions,model_predictions_ada).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'learning_rate': 1, 'loss': 'linear', 'n_estimators': 2}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf_ada.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1522876.0\n"
     ]
    }
   ],
   "source": [
    "regressor = AdaBoostRegressor(n_estimators = 200, loss = 'square')\n",
    "estimator_ada = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_ada.fit(X_train, y_train)\n",
    "pred_ada = estimator_ada.predict(X_test)\n",
    "model_predictions_ada = convertToPrediction(y_for_calculations,pred_ada)\n",
    "print(profit(actual_predictions,model_predictions_ada).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE Value:  0.47134455308457096\n"
     ]
    }
   ],
   "source": [
    "print (\"RMSLE Value: \",rmsle(actual_predictions,model_predictions_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation metrics\n",
    "from sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor\n",
      "1584083.0\n",
      "GradientBoostingRegressor\n",
      "1581123.0\n",
      "AdaBoostRegressor\n",
      "1509126.0\n",
      "BaggingRegressor\n",
      "1595757.0\n",
      "SVR\n",
      "1438256.0\n",
      "KNeighborsRegressor\n",
      "1445710.0\n"
     ]
    }
   ],
   "source": [
    "models=[RandomForestRegressor(),GradientBoostingRegressor(),AdaBoostRegressor(),BaggingRegressor(),SVR(),KNeighborsRegressor()]\n",
    "model_names=['RandomForestRegressor','GradientBoostingRegressor','AdaBoostRegressor','BaggingRegressor','SVR','KNeighborsRegressor']\n",
    "rmsle=[]\n",
    "d={}\n",
    "for model in range (len(models)):\n",
    "    clf=models[model]\n",
    "    print(model_names[model])\n",
    "    clf.fit(X_train,y_train)\n",
    "    test_pred=clf.predict(X_test)\n",
    "    model_predictions = convertToPrediction(y_for_calculations,test_pred)\n",
    "    print(profit(actual_predictions,model_predictions).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW LET'S Dig deeper into each of these ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The regressors are trained using randomized search and cross-validation to identify the area of the best parameters. Then a grid search is used to tune parameter values of the regressor functions.\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548673.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for random forest regresion.\n",
    "no_of_test=[500]\n",
    "params_dict={'n_estimators':no_of_test,'n_jobs':[-1],'max_features':[\"auto\",'sqrt','log2'],'max_depth':[10,20,30]}\n",
    "clf_rf=GridSearchCV(estimator=RandomForestRegressor(),param_grid=params_dict,scoring='neg_mean_squared_error')\n",
    "clf_rf.fit(X_train,y_train)\n",
    "pred_rf=clf_rf.predict(X_test)\n",
    "model_predictions_rf = convertToPrediction(y_for_calculations,pred_rf)\n",
    "print(profit(actual_predictions,model_predictions_rf).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 500, 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1598299.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "#regressor = RandomForestRegressor(random_state = 0, max_depth = 30, n_estimators = 500, max_features = 'log2')\n",
    "regressor = RandomForestRegressor()\n",
    "estimator_rf = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_rf.fit(X_train, y_train)\n",
    "pred_rf = estimator_rf.predict(X_test)\n",
    "model_predictions_rf = convertToPrediction(y_for_calculations,pred_rf)\n",
    "print(profit(actual_predictions,model_predictions_rf).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520967.0\n"
     ]
    }
   ],
   "source": [
    "#for Gradient Boosting regresion.\n",
    "no_of_estimators=[100,200,300,400,500]\n",
    "params_dict={'n_estimators':no_of_estimators,'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "             'max_features':[\"auto\",'sqrt','log2'],'max_depth':[10,20,30,40,50]}\n",
    "clf_gbr=GridSearchCV(estimator=GradientBoostingRegressor(),param_grid=params_dict,scoring='neg_mean_squared_error')\n",
    "clf_gbr.fit(X_train,y_train)\n",
    "pred=clf_gbr.predict(X_test)\n",
    "model_predictions = convertToPrediction(y_for_calculations,pred)\n",
    "print(profit(actual_predictions,model_predictions).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'learning_rate': 0.3, 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf_gbr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1581854.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#gbr = GradientBoostingRegressor(n_estimators=500,max_features= 'log2', learning_rate=0.1, max_depth = 10)\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "estimator_gbr = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', gbr)\n",
    "    ]\n",
    ")\n",
    "estimator_gbr.fit(X_train, y_train)\n",
    "pred_gbr = estimator_gbr.predict(X_test)\n",
    "model_predictions_gbr = convertToPrediction(y_for_calculations,pred_gbr)\n",
    "print(profit(actual_predictions,model_predictions_gbr).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADA Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521528.0\n"
     ]
    }
   ],
   "source": [
    "### ADA Boost Regressor\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "#regressor = RandomForestRegressor(random_state = 0, max_depth = 30, n_estimators = 500, max_features = 'log2')\n",
    "regressor = AdaBoostRegressor()\n",
    "estimator_ada = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_ada.fit(X_train, y_train)\n",
    "pred_ada = estimator_ada.predict(X_test)\n",
    "model_predictions_ada = convertToPrediction(y_for_calculations,pred_ada)\n",
    "print(profit(actual_predictions,model_predictions_ada).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584347.0\n"
     ]
    }
   ],
   "source": [
    "### Bagging Regressor\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "#regressor = RandomForestRegressor(random_state = 0, max_depth = 30, n_estimators = 500, max_features = 'log2')\n",
    "regressor = BaggingRegressor()\n",
    "estimator_bagging = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_bagging.fit(X_train, y_train)\n",
    "pred_bagging = estimator_bagging.predict(X_test)\n",
    "model_predictions_bagging = convertToPrediction(y_for_calculations,pred_bagging)\n",
    "print(profit(actual_predictions,model_predictions_bagging).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1445710.0\n"
     ]
    }
   ],
   "source": [
    "### KNN Regressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "\n",
    "regressor = KNeighborsRegressor()\n",
    "estimator_knn = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_knn.fit(X_train, y_train)\n",
    "pred_knn = estimator_knn.predict(X_test)\n",
    "model_predictions_knn = convertToPrediction(y_for_calculations,pred_knn)\n",
    "print(profit(actual_predictions,model_predictions_knn).sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training SVR\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score SVR: -0.003305\n",
      "RMSE SVR: 1318.397271\n"
     ]
    }
   ],
   "source": [
    "# Validation SVR\n",
    "\n",
    "pred_svr = svr.predict(X_test)\n",
    "score_svr = r2_score(y_test, pred_svr)\n",
    "rmse_svr = sqrt(mean_squared_error(y_test, pred_svr))\n",
    "\n",
    "print(\"Score SVR: %f\" % score_svr)\n",
    "print(\"RMSE SVR: %f\" % rmse_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid=[{'C': [1000, 3000, 10000], 'kernel': ['linear', 'rbf']}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)\n",
      "\n",
      "Best parameter from grid search: {'C': 1000, 'kernel': 'rbf'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tuning SVR with GridSearch\n",
    "\n",
    "tuned_parameters = [{'C': [1000, 3000, 10000], \n",
    "                     'kernel': ['linear', 'rbf']}\n",
    "                   ]\n",
    "\n",
    "#svr_tuned = GridSearchCV(SVR (C=1), param_grid = tuned_parameters, scoring = 'mean_squared_error') #default 3-fold cross-validation, score method of the estimator\n",
    "svr_tuned_GS = GridSearchCV(SVR (C=1), param_grid = tuned_parameters, scoring = 'r2', n_jobs=-1) #default 3-fold cross-validation, score method of the estimator\n",
    "\n",
    "svr_tuned_GS.fit(X_train, y_train)\n",
    "\n",
    "print (svr_tuned_GS)\n",
    "print ('\\n' \"Best parameter from grid search: \" + str(svr_tuned_GS.best_params_) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Results\n",
      "\n",
      "Score SVR: -0.003305\n",
      "Score SVR tuned GS: -0.002542\n",
      "\n",
      "RMSE SVR: 1318.397271\n",
      "RMSE SVR tuned GS: 1317.895584\n"
     ]
    }
   ],
   "source": [
    "svr_tuned_pred_GS = svr_tuned_GS.predict(X_test)\n",
    "\n",
    "score_svr_tuned_GS = r2_score(y_test, svr_tuned_pred_GS)\n",
    "rmse_svr_tuned_GS = sqrt(mean_squared_error(y_test, svr_tuned_pred_GS))\n",
    "\n",
    "print(\"SVR Results\\n\")\n",
    "\n",
    "print(\"Score SVR: %f\" % score_svr)\n",
    "print(\"Score SVR tuned GS: %f\" % score_svr_tuned_GS)\n",
    "\n",
    "print(\"\\nRMSE SVR: %f\" % rmse_svr)\n",
    "print(\"RMSE SVR tuned GS: %f\" % rmse_svr_tuned_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1269779.0\n"
     ]
    }
   ],
   "source": [
    "svr_tuned_pred_GS\n",
    "\n",
    "\n",
    "##Profit Calculation for pct approach\n",
    "model_predictions = convertToPrediction(y_for_calculations,svr_tuned_pred_GS)\n",
    "print(profit(actual_predictions,model_predictions).sum())\n",
    "\n",
    "#Profit is just 1.26million!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best CV score from grid search: -0.034399\n",
      "corresponding parameters: {'C': 4289.95475119251, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# SVR tuned with RandomizesSearch\n",
    "# may take a while!\n",
    "\n",
    "# Parameters\n",
    "param_dist = {  'C': sp_uniform (1000, 10000), \n",
    "                'kernel': ['rbf']\n",
    "             }\n",
    "\n",
    "n_iter_search = 1\n",
    "\n",
    "# MSE optimized\n",
    "#SVR_tuned_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'mean_squared_error', n_iter=n_iter_search)\n",
    "\n",
    "# R^2 optimized\n",
    "SVR_tuned_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'r2', n_iter=n_iter_search)\n",
    "\n",
    "# Fit\n",
    "SVR_tuned_RS.fit(X_train, y_train)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('best CV score from grid search: {0:f}'.format(SVR_tuned_RS.best_score_))\n",
    "print('corresponding parameters: {}'.format(SVR_tuned_RS.best_params_))\n",
    "\n",
    "# Predict and score\n",
    "predict = SVR_tuned_RS.predict(X_test)\n",
    "\n",
    "score_svr_tuned_RS = r2_score(y_test, predict)\n",
    "rmse_svr_tuned_RS = sqrt(mean_squared_error(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Results\n",
      "\n",
      "Score SVR: -0.003305\n",
      "Score SVR tuned GS: -0.002542\n",
      "Score SVR tuned RS: -0.002542\n",
      "\n",
      "RMSE SVR: 1318.397271\n",
      "RMSE SVR tuned GS: 1317.895584\n",
      "RMSE SVR tuned RS: 1317.895584\n"
     ]
    }
   ],
   "source": [
    "print('SVR Results\\n')\n",
    "\n",
    "print(\"Score SVR: %f\" % score_svr)\n",
    "print(\"Score SVR tuned GS: %f\" % score_svr_tuned_GS)\n",
    "print(\"Score SVR tuned RS: %f\" % score_svr_tuned_RS)\n",
    "\n",
    "print(\"\\nRMSE SVR: %f\" % rmse_svr)\n",
    "print(\"RMSE SVR tuned GS: %f\" % rmse_svr_tuned_GS)\n",
    "print(\"RMSE SVR tuned RS: %f\" % rmse_svr_tuned_RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1269779.0\n"
     ]
    }
   ],
   "source": [
    "##Profit Calculation for pct approach\n",
    "model_predictions = convertToPrediction(y_for_calculations,predict)\n",
    "print(profit(actual_predictions,model_predictions).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from time import time\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_model = MLPRegressor(hidden_layer_sizes=(5,),\n",
    "                                       activation='relu',\n",
    "                                       solver='adam',\n",
    "                                       learning_rate='adaptive',\n",
    "                                       max_iter=15000,\n",
    "                                       learning_rate_init=0.01,\n",
    "                                       alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = int(time() * 1000)\n",
    "bike_model.fit(X_train, y_train)\n",
    "end_time = int(time() * 1000)\n",
    "logging.debug('Finished training universal model')\n",
    "logging.debug('Training took {} ms'.format(end_time - start_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dnn = bike_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions_dnn = convertToPrediction(y_for_calculations,pred_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_predictions_dnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-e784ea2c3543>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_predictions_dnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_predictions_dnn' is not defined"
     ]
    }
   ],
   "source": [
    "print(profit(actual_predictions,model_predictions_dnn).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averageing best predictions from different regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1606967.0\n"
     ]
    }
   ],
   "source": [
    "w = [1, 1, 1, 1]  # weights\n",
    "pred_agg = np.c_[pred_rf,pred_gbr,pred_ada,pred_bagging]\n",
    "pred_avr = np.average(pred_agg, axis=1, weights=w)\n",
    "model_predictions_avr = convertToPrediction(y_for_calculations,pred_avr)\n",
    "print(profit(actual_predictions,model_predictions_avr).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble : Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_rf = estimator_rf.predict(X_train)\n",
    "pred_train_gbr = estimator_gbr.predict(X_train)\n",
    "pred_train_ada = estimator_ada.predict(X_train)\n",
    "pred_train_bagging = estimator_bagging.predict(X_train)\n",
    "pred_train_knn = estimator_knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = pd.DataFrame({'rf':pred_train_rf, 'gbr':pred_train_gbr, 'ada':pred_train_ada,\n",
    "                      'bagging':pred_train_bagging,'true':y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/qk/0vhw5p4n2610vyrs2zqjmj2h0000gn/T/tmp6hans2jg\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11df38a58>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/var/folders/qk/0vhw5p4n2610vyrs2zqjmj2h0000gn/T/tmp6hans2jg'}\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/qk/0vhw5p4n2610vyrs2zqjmj2h0000gn/T/tmp6hans2jg/model.ckpt.\n",
      "INFO:tensorflow:loss = 3059.3237, step = 1\n",
      "INFO:tensorflow:global_step/sec: 237.819\n",
      "INFO:tensorflow:loss = 75.17607, step = 101 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.751\n",
      "INFO:tensorflow:loss = 65.99632, step = 201 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.023\n",
      "INFO:tensorflow:loss = 59.48324, step = 301 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.904\n",
      "INFO:tensorflow:loss = 59.947117, step = 401 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.187\n",
      "INFO:tensorflow:loss = 52.028896, step = 501 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.322\n",
      "INFO:tensorflow:loss = 57.540497, step = 601 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.961\n",
      "INFO:tensorflow:loss = 49.29046, step = 701 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.5\n",
      "INFO:tensorflow:loss = 49.66929, step = 801 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.529\n",
      "INFO:tensorflow:loss = 49.200485, step = 901 (0.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 269.226\n",
      "INFO:tensorflow:loss = 48.84284, step = 1001 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.765\n",
      "INFO:tensorflow:loss = 42.83662, step = 1101 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.554\n",
      "INFO:tensorflow:loss = 52.783813, step = 1201 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.659\n",
      "INFO:tensorflow:loss = 45.701065, step = 1301 (0.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.834\n",
      "INFO:tensorflow:loss = 49.443386, step = 1401 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.997\n",
      "INFO:tensorflow:loss = 45.19738, step = 1501 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.784\n",
      "INFO:tensorflow:loss = 42.31507, step = 1601 (0.366 sec)\n",
      "INFO:tensorflow:global_step/sec: 278.173\n",
      "INFO:tensorflow:loss = 46.46778, step = 1701 (0.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 275.102\n",
      "INFO:tensorflow:loss = 44.424095, step = 1801 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 275.75\n",
      "INFO:tensorflow:loss = 45.28433, step = 1901 (0.363 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.699\n",
      "INFO:tensorflow:loss = 44.8379, step = 2001 (0.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 279.321\n",
      "INFO:tensorflow:loss = 46.773308, step = 2101 (0.358 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.172\n",
      "INFO:tensorflow:loss = 43.591244, step = 2201 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 279.885\n",
      "INFO:tensorflow:loss = 39.95231, step = 2301 (0.358 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.543\n",
      "INFO:tensorflow:loss = 38.855755, step = 2401 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.607\n",
      "INFO:tensorflow:loss = 42.83182, step = 2501 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.805\n",
      "INFO:tensorflow:loss = 42.762466, step = 2601 (0.421 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.564\n",
      "INFO:tensorflow:loss = 40.52874, step = 2701 (0.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.076\n",
      "INFO:tensorflow:loss = 39.438393, step = 2801 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.549\n",
      "INFO:tensorflow:loss = 34.351116, step = 2901 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.249\n",
      "INFO:tensorflow:loss = 40.897224, step = 3001 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.501\n",
      "INFO:tensorflow:loss = 39.30667, step = 3101 (0.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.848\n",
      "INFO:tensorflow:loss = 34.749588, step = 3201 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.112\n",
      "INFO:tensorflow:loss = 37.229134, step = 3301 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.485\n",
      "INFO:tensorflow:loss = 35.205765, step = 3401 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.158\n",
      "INFO:tensorflow:loss = 39.650482, step = 3501 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.909\n",
      "INFO:tensorflow:loss = 30.82904, step = 3601 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.576\n",
      "INFO:tensorflow:loss = 35.16824, step = 3701 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.23\n",
      "INFO:tensorflow:loss = 32.439182, step = 3801 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.236\n",
      "INFO:tensorflow:loss = 36.866516, step = 3901 (0.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 274.625\n",
      "INFO:tensorflow:loss = 36.22116, step = 4001 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 269.517\n",
      "INFO:tensorflow:loss = 32.023598, step = 4101 (0.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.977\n",
      "INFO:tensorflow:loss = 37.233852, step = 4201 (0.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 279.88\n",
      "INFO:tensorflow:loss = 36.03682, step = 4301 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 278.184\n",
      "INFO:tensorflow:loss = 27.798252, step = 4401 (0.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 275.484\n",
      "INFO:tensorflow:loss = 32.421707, step = 4501 (0.363 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.286\n",
      "INFO:tensorflow:loss = 32.882843, step = 4601 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 279.976\n",
      "INFO:tensorflow:loss = 33.217384, step = 4701 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 275.902\n",
      "INFO:tensorflow:loss = 35.68681, step = 4801 (0.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 278.121\n",
      "INFO:tensorflow:loss = 33.388077, step = 4901 (0.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 277.026\n",
      "INFO:tensorflow:loss = 34.64009, step = 5001 (0.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 277.403\n",
      "INFO:tensorflow:loss = 39.931763, step = 5101 (0.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 277.465\n",
      "INFO:tensorflow:loss = 29.72985, step = 5201 (0.361 sec)\n",
      "INFO:tensorflow:global_step/sec: 277.689\n",
      "INFO:tensorflow:loss = 31.493162, step = 5301 (0.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 279.173\n",
      "INFO:tensorflow:loss = 33.204315, step = 5401 (0.358 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.567\n",
      "INFO:tensorflow:loss = 32.402378, step = 5501 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 276.036\n",
      "INFO:tensorflow:loss = 32.500435, step = 5601 (0.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 279.85\n",
      "INFO:tensorflow:loss = 31.325394, step = 5701 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.352\n",
      "INFO:tensorflow:loss = 32.49613, step = 5801 (0.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.577\n",
      "INFO:tensorflow:loss = 29.188797, step = 5901 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.32\n",
      "INFO:tensorflow:loss = 30.879389, step = 6001 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.879\n",
      "INFO:tensorflow:loss = 29.713264, step = 6101 (0.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.206\n",
      "INFO:tensorflow:loss = 31.87378, step = 6201 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.472\n",
      "INFO:tensorflow:loss = 27.62133, step = 6301 (0.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 271.435\n",
      "INFO:tensorflow:loss = 24.727306, step = 6401 (0.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.158\n",
      "INFO:tensorflow:loss = 30.967487, step = 6501 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.907\n",
      "INFO:tensorflow:loss = 28.71612, step = 6601 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.016\n",
      "INFO:tensorflow:loss = 28.27856, step = 6701 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.904\n",
      "INFO:tensorflow:loss = 27.201637, step = 6801 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.431\n",
      "INFO:tensorflow:loss = 31.163668, step = 6901 (0.438 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 249.36\n",
      "INFO:tensorflow:loss = 28.26818, step = 7001 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.17\n",
      "INFO:tensorflow:loss = 29.699017, step = 7101 (0.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.299\n",
      "INFO:tensorflow:loss = 30.459818, step = 7201 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.489\n",
      "INFO:tensorflow:loss = 28.183102, step = 7301 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.228\n",
      "INFO:tensorflow:loss = 31.609444, step = 7401 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.041\n",
      "INFO:tensorflow:loss = 27.979221, step = 7501 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.562\n",
      "INFO:tensorflow:loss = 24.523727, step = 7601 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.298\n",
      "INFO:tensorflow:loss = 23.544832, step = 7701 (0.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.173\n",
      "INFO:tensorflow:loss = 25.875946, step = 7801 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 281.2\n",
      "INFO:tensorflow:loss = 26.609013, step = 7901 (0.356 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8000 into /var/folders/qk/0vhw5p4n2610vyrs2zqjmj2h0000gn/T/tmp6hans2jg/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 30.801643.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNRegressor(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._RegressionHead object at 0x10d672c50>, 'hidden_units': [50, 20], 'feature_columns': (_RealValuedColumn(column_name='', dimension=4, default_value=None, dtype=tf.float64, normalizer=None),), 'optimizer': <tensorflow.python.training.adam.AdamOptimizer object at 0x10d672ba8>, 'activation_fn': <function leaky_relu at 0x10f726f28>, 'dropout': None, 'gradient_clip_norm': None, 'embedding_lr_multipliers': None, 'input_layer_min_slice_size': None})"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "import tensorflow as tf\n",
    "\n",
    "# Stacking multiple regressors using DNN\n",
    "features_blend = learn.infer_real_valued_columns_from_input(stack[['rf','gbr','ada','bagging']])\n",
    "\n",
    "# Optimizer algorithm\n",
    "adam = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "# Build multi-layer DNN for regression\n",
    "blend_nn = learn.DNNRegressor(feature_columns=features_blend, hidden_units=[50, 20], \n",
    "                              optimizer=adam, activation_fn=tf.nn.leaky_relu)\n",
    "#                               config=learn.estimators.RunConfig(num_cores=8))\n",
    "# Fit DNN\n",
    "blend_nn.fit(x=stack[['rf','gbr','ada','bagging']], y=stack['true'], steps=8000, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT: RF for Regression\n",
    "y_test_rf = estimator_rf.predict(X_test).flatten().tolist()\n",
    "\n",
    "# PREDICT: GBR \n",
    "y_test_gbr = estimator_gbr.predict(X_test).flatten().tolist()\n",
    "\n",
    "# PREDICT: ADA\n",
    "y_test_ada = estimator_ada.predict(X_test).flatten().tolist()\n",
    "\n",
    "# PREDICT: BAGGING\n",
    "y_test_bagging = estimator_bagging.predict(X_test).flatten().tolist()\n",
    "\n",
    "# PREDICT: KNN\n",
    "y_test_knn = estimator_knn.predict(X_test).flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stack = pd.DataFrame(data={'rf':y_test_rf, 'gbr':y_test_gbr,\n",
    "                               'ada':y_test_ada,'bagging': y_test_bagging,\n",
    "                               'true':y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/qk/0vhw5p4n2610vyrs2zqjmj2h0000gn/T/tmp6hans2jg/model.ckpt-8000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "y_new_bl = list(blend_nn.predict(new_stack[['rf', 'gbr', 'ada','bagging']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1576722.0\n"
     ]
    }
   ],
   "source": [
    "model_predictions_ensemble = convertToPrediction(y_for_calculations,y_new_bl)\n",
    "print(profit(actual_predictions,model_predictions_ensemble).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
